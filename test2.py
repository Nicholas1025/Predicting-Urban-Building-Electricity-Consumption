"""
ä¿®å¤æ•°æ®ä¸¢å¤±é—®é¢˜
fix_data_loss.py - è¯Šæ–­å¹¶ä¿®å¤é¢„å¤„ç†ä¸­çš„æ•°æ®ä¸¢å¤±
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os

def debug_preprocess_data(file_path, test_size=0.2, random_state=42):
    """
    å¸¦è°ƒè¯•ä¿¡æ¯çš„é¢„å¤„ç†å‡½æ•°ï¼Œæ‰¾å‡ºæ•°æ®ä¸¢å¤±çš„åœ°æ–¹
    """
    print(f"ğŸ” è°ƒè¯•é¢„å¤„ç†: {file_path}")
    
    # 1. åŠ è½½æ•°æ®
    df = pd.read_csv(file_path)
    print(f"1ï¸âƒ£ åŠ è½½åŸå§‹æ•°æ®: {df.shape}")
    
    # 2. è¯†åˆ«æ•°æ®é›†ç±»å‹
    from preprocessing.clean_data import identify_dataset_type, preprocess_data_city_specific
    dataset_type = identify_dataset_type(df, file_path)
    print(f"2ï¸âƒ£ æ•°æ®é›†ç±»å‹: {dataset_type}")
    
    # 3. åŸå¸‚ç‰¹å®šé¢„å¤„ç†
    df_processed, target_col = preprocess_data_city_specific(df, dataset_type)
    print(f"3ï¸âƒ£ åŸå¸‚é¢„å¤„ç†å: {df_processed.shape}")
    print(f"    ç›®æ ‡å˜é‡: {target_col}")
    
    # 4. åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
    y = df_processed[target_col].copy()
    X = df_processed.drop(columns=[target_col])
    print(f"4ï¸âƒ£ åˆ†ç¦»ç‰¹å¾ç›®æ ‡: X={X.shape}, y={y.shape}")
    
    # 5. åˆ é™¤æ— å…³åˆ—
    from preprocessing.clean_data import get_fair_city_config
    config = get_fair_city_config(dataset_type)
    irrelevant_cols = config['irrelevant_cols']
    
    existing_irrelevant = [col for col in irrelevant_cols if col in X.columns]
    if existing_irrelevant:
        X = X.drop(columns=existing_irrelevant)
        print(f"5ï¸âƒ£ åˆ é™¤æ— å…³åˆ—å: {X.shape} (åˆ é™¤äº† {len(existing_irrelevant)} åˆ—)")
    
    # 6. å¤„ç†åˆ†ç±»å˜é‡
    from sklearn.preprocessing import LabelEncoder
    categorical_cols = X.select_dtypes(include=['object']).columns
    
    print(f"6ï¸âƒ£ å¤„ç†åˆ†ç±»å˜é‡: æ‰¾åˆ° {len(categorical_cols)} ä¸ªåˆ†ç±»åˆ—")
    
    for col in categorical_cols:
        unique_count = X[col].nunique()
        if unique_count > 100:
            X = X.drop(columns=[col])
            print(f"    åˆ é™¤ {col} - ç±»åˆ«å¤ªå¤š ({unique_count})")
        elif unique_count <= 1:
            X = X.drop(columns=[col])
            print(f"    åˆ é™¤ {col} - å¸¸æ•°åˆ—")
        else:
            le = LabelEncoder()
            X[col] = X[col].fillna('Unknown')
            X[col] = le.fit_transform(X[col].astype(str))
            print(f"    ç¼–ç  {col} - {unique_count} ä¸ªç±»åˆ«")
    
    print(f"6ï¸âƒ£ åˆ†ç±»å¤„ç†å: {X.shape}")
    
    # 7. å¤„ç†æ•°å€¼å˜é‡çš„ç¼ºå¤±å€¼
    numerical_cols = X.select_dtypes(include=[np.number]).columns
    if len(numerical_cols) > 0:
        missing_before = X[numerical_cols].isnull().sum().sum()
        for col in numerical_cols:
            if X[col].isnull().sum() > 0:
                median_val = X[col].median()
                X[col] = X[col].fillna(median_val if pd.notna(median_val) else 0)
        missing_after = X[numerical_cols].isnull().sum().sum()
        print(f"7ï¸âƒ£ å¤„ç†ç¼ºå¤±å€¼: {missing_before} â†’ {missing_after}")
    
    # 8. åˆ é™¤å¸¸æ•°åˆ—
    constant_cols = [col for col in X.columns if X[col].nunique() <= 1]
    if constant_cols:
        X = X.drop(columns=constant_cols)
        print(f"8ï¸âƒ£ åˆ é™¤å¸¸æ•°åˆ—: {len(constant_cols)} åˆ—ï¼Œå‰©ä½™ {X.shape}")
    
    # 9. åˆ é™¤é«˜ç›¸å…³æ€§ç‰¹å¾
    if len(X.columns) > 1:
        try:
            corr_matrix = X.corr().abs()
            upper_triangle = corr_matrix.where(
                np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
            )
            high_corr_features = [column for column in upper_triangle.columns 
                                 if any(upper_triangle[column] > 0.95)]
            if high_corr_features:
                X = X.drop(columns=high_corr_features)
                print(f"9ï¸âƒ£ åˆ é™¤é«˜ç›¸å…³ç‰¹å¾: {len(high_corr_features)} åˆ—ï¼Œå‰©ä½™ {X.shape}")
        except Exception as e:
            print(f"9ï¸âƒ£ ç›¸å…³æ€§åˆ†æå¤±è´¥: {e}")
    
    print(f"ğŸ”Ÿ æœ€ç»ˆç‰¹å¾æ•°é‡: {X.shape[1]}")
    
    if X.shape[1] == 0:
        print("âŒ æ‰€æœ‰ç‰¹å¾éƒ½è¢«åˆ é™¤äº†!")
        return None
    
    # ğŸš¨ è¿™é‡Œå¯èƒ½æ˜¯é—®é¢˜æ‰€åœ¨ - æ£€æŸ¥æ•°æ®å¯¹é½
    print(f"ğŸ“Š æ£€æŸ¥æ•°æ®å¯¹é½:")
    print(f"   Xç´¢å¼•èŒƒå›´: {X.index.min()} - {X.index.max()}")
    print(f"   yç´¢å¼•èŒƒå›´: {y.index.min()} - {y.index.max()}")
    
    # ç¡®ä¿Xå’Œyç´¢å¼•å¯¹é½
    common_indices = X.index.intersection(y.index)
    if len(common_indices) != len(X):
        print(f"âš ï¸ ç´¢å¼•ä¸å¯¹é½! å…¬å…±ç´¢å¼•: {len(common_indices)}, Xé•¿åº¦: {len(X)}")
        X = X.loc[common_indices]
        y = y.loc[common_indices]
        print(f"ğŸ“Š å¯¹é½å: X={X.shape}, y={y.shape}")
    
    # 10. æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    print(f"ğŸ”„ æ•°æ®åˆ†å‰²:")
    print(f"   è®­ç»ƒé›†: X={X_train.shape}, y={y_train.shape}")
    print(f"   æµ‹è¯•é›†: X={X_test.shape}, y={y_test.shape}")
    
    # 11. æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # è½¬æ¢å›DataFrame
    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)
    
    print(f"ğŸ“ æ ‡å‡†åŒ–å®Œæˆ:")
    print(f"   X_train: {X_train_scaled.shape}")
    print(f"   X_test: {X_test_scaled.shape}")
    
    feature_names = X_train.columns.tolist()
    
    print(f"âœ… é¢„å¤„ç†å®Œæˆ!")
    print(f"   æœ€ç»ˆè®­ç»ƒé›†: {X_train_scaled.shape[0]} æ ·æœ¬")
    print(f"   æœ€ç»ˆæµ‹è¯•é›†: {X_test_scaled.shape[0]} æ ·æœ¬")
    print(f"   ç‰¹å¾æ•°é‡: {len(feature_names)}")
    
    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, feature_names

def fix_preprocessing_for_dataset(dataset_name, file_path):
    """
    ä¿®å¤ç‰¹å®šæ•°æ®é›†çš„é¢„å¤„ç†é—®é¢˜
    """
    print(f"\nğŸ”§ ä¿®å¤ {dataset_name} çš„é¢„å¤„ç†...")
    
    try:
        # ä½¿ç”¨è°ƒè¯•é¢„å¤„ç†
        result = debug_preprocess_data(file_path)
        
        if result is None:
            print(f"âŒ {dataset_name} é¢„å¤„ç†å¤±è´¥")
            return False
        
        X_train, X_test, y_train, y_test, scaler, feature_names = result
        
        # ä¿å­˜åˆ°æ­£ç¡®çš„ç›®å½•
        output_dir = f"outputs/{dataset_name}"
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜æ•°æ®
        X_train.to_csv(f"{output_dir}/X_train.csv", index=False)
        X_test.to_csv(f"{output_dir}/X_test.csv", index=False)
        y_train.to_csv(f"{output_dir}/y_train.csv", index=False)
        y_test.to_csv(f"{output_dir}/y_test.csv", index=False)
        
        # ä¿å­˜ç‰¹å¾åç§°
        with open(f"{output_dir}/feature_names.txt", "w") as f:
            for name in feature_names:
                f.write(f"{name}\n")
        
        # ä¿å­˜scaler
        import joblib
        joblib.dump(scaler, f"{output_dir}/scaler.pkl")
        
        print(f"âœ… {dataset_name} ä¿®å¤å®Œæˆ!")
        print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
        print(f"   ç‰¹å¾: {len(feature_names)} ä¸ª")
        
        return True
        
    except Exception as e:
        print(f"âŒ {dataset_name} ä¿®å¤å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def fix_all_datasets():
    """
    ä¿®å¤æ‰€æœ‰æ•°æ®é›†çš„æ•°æ®ä¸¢å¤±é—®é¢˜
    """
    print("ğŸš€ å¼€å§‹ä¿®å¤æ‰€æœ‰æ•°æ®é›†çš„æ•°æ®ä¸¢å¤±é—®é¢˜...")
    
    datasets = {
        'seattle_2015_present': 'data/seattle_2015_present.csv',
        'chicago_energy': 'data/chicago_energy_benchmarking.csv',
        'washington_dc': 'data/washington_dc_energy.csv'
    }
    
    results = {}
    
    for dataset_name, file_path in datasets.items():
        if os.path.exists(file_path):
            success = fix_preprocessing_for_dataset(dataset_name, file_path)
            results[dataset_name] = success
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            results[dataset_name] = False
    
    # æ€»ç»“
    print(f"\nğŸ“Š ä¿®å¤ç»“æœ:")
    successful = sum(results.values())
    total = len(results)
    
    for dataset, success in results.items():
        status = "âœ…" if success else "âŒ"
        print(f"   {status} {dataset}")
    
    print(f"\nğŸ¯ æˆåŠŸç‡: {successful}/{total} ({successful/total*100:.0f}%)")
    
    if successful > 0:
        print(f"\nğŸ’¡ å»ºè®®:")
        print(f"1. é‡æ–°è¿è¡Œæ¨¡å‹è®­ç»ƒ:")
        for dataset, success in results.items():
            if success:
                print(f"   python models/train_xgboost_individual.py {dataset}")
                print(f"   python models/train_rf_individual.py {dataset}")
                print(f"   python models/train_svr_individual.py {dataset}")
        
        print(f"2. é‡æ–°è¿è¡Œè¯„ä¼°:")
        for dataset, success in results.items():
            if success:
                print(f"   python evaluation/evaluate_models_individual.py {dataset}")
    
    return results

if __name__ == "__main__":
    fix_all_datasets()