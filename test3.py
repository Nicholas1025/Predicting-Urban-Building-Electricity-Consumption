"""
ä¿®å¤å¼‚å¸¸å€¼ç§»é™¤é—®é¢˜çš„è¡¥ä¸
fix_outlier_removal.py
"""

import pandas as pd
import numpy as np
import os

def apply_fixed_outlier_removal(df, dataset_type, target_col):
    """
    ä¿®å¤åŽçš„å¼‚å¸¸å€¼ç§»é™¤å‡½æ•° - åªå¯¹ç›®æ ‡å˜é‡è¿›è¡Œå¼‚å¸¸å€¼ç§»é™¤
    """
    from preprocessing.clean_data import get_fair_city_config
    config = get_fair_city_config(dataset_type)
    low_pct, high_pct = config['outlier_percentiles']
    
    print(f"{dataset_type}: åº”ç”¨ä¿®å¤çš„å¼‚å¸¸å€¼ç§»é™¤ ({low_pct}%-{high_pct}% èŒƒå›´)")
    
    original_shape = df.shape[0]
    
    # ðŸ”§ ä¿®å¤: åªå¯¹ç›®æ ‡å˜é‡è¿›è¡Œå¼‚å¸¸å€¼ç§»é™¤ï¼Œä¸å¯¹å…¶ä»–åˆ—è¿›è¡Œè¿‡æ»¤
    if target_col in df.columns:
        target_data = pd.to_numeric(df[target_col], errors='coerce')
        q_low = target_data.quantile(low_pct / 100)
        q_high = target_data.quantile(high_pct / 100)
        
        # åªç§»é™¤ç›®æ ‡å˜é‡çš„æžç«¯å¼‚å¸¸å€¼
        target_mask = (target_data >= q_low) & (target_data <= q_high)
        df = df[target_mask]
        
        print(f"ç›®æ ‡å˜é‡å¼‚å¸¸å€¼ç§»é™¤: ä¿ç•™ {df.shape[0]}/{original_shape} è¡Œ")
    
    # ðŸš« åˆ é™¤å¯¹å…¶ä»–æ•°å€¼åˆ—çš„å¼‚å¸¸å€¼å¤„ç† - è¿™æ˜¯å¯¼è‡´é—®é¢˜çš„æ ¹æº
    # ä¸å†å¯¹æ¯ä¸ªæ•°å€¼åˆ—å•ç‹¬è¿›è¡Œå¼‚å¸¸å€¼ç§»é™¤
    
    final_shape = df.shape[0]
    removed_count = original_shape - final_shape
    removal_pct = (removed_count / original_shape) * 100
    
    print(f"æ€»å¼‚å¸¸å€¼ç§»é™¤: {removed_count} è¡Œ ({removal_pct:.1f}%)")
    
    return df

def patch_clean_data_module():
    """
    ä¿®è¡¥ clean_data.py æ¨¡å—ä¸­çš„å¼‚å¸¸å€¼ç§»é™¤å‡½æ•°
    """
    print("ðŸ”§ æ­£åœ¨ä¿®è¡¥ clean_data.py æ¨¡å—...")
    
    try:
        # å¯¼å…¥æ¨¡å—
        import preprocessing.clean_data as clean_data_module
        
        # æ›¿æ¢å‡½æ•°
        clean_data_module.apply_fair_outlier_removal = apply_fixed_outlier_removal
        
        print("âœ… å¼‚å¸¸å€¼ç§»é™¤å‡½æ•°å·²ä¿®è¡¥")
        return True
        
    except Exception as e:
        print(f"âŒ ä¿®è¡¥å¤±è´¥: {e}")
        return False

def reprocess_dataset_with_fix(dataset_name, file_path):
    """
    ä½¿ç”¨ä¿®å¤åŽçš„å¼‚å¸¸å€¼ç§»é™¤é‡æ–°å¤„ç†æ•°æ®é›†
    """
    print(f"\nðŸ”§ ä½¿ç”¨ä¿®å¤åŽçš„æ–¹æ³•é‡æ–°å¤„ç† {dataset_name}...")
    
    # å…ˆä¿®è¡¥æ¨¡å—
    if not patch_clean_data_module():
        return False
    
    try:
        # ä½¿ç”¨ä¿®å¤åŽçš„é¢„å¤„ç†
        from preprocessing.clean_data import preprocess_data
        
        result = preprocess_data(file_path)
        
        if result is None:
            print(f"âŒ {dataset_name} é¢„å¤„ç†å¤±è´¥")
            return False
        
        X_train, X_test, y_train, y_test, scaler, feature_names = result
        
        # ä¿å­˜åˆ°æ­£ç¡®çš„ç›®å½•
        output_dir = f"outputs/{dataset_name}"
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜æ•°æ®
        X_train.to_csv(f"{output_dir}/X_train.csv", index=False)
        X_test.to_csv(f"{output_dir}/X_test.csv", index=False)
        y_train.to_csv(f"{output_dir}/y_train.csv", index=False)  
        y_test.to_csv(f"{output_dir}/y_test.csv", index=False)
        
        # ä¿å­˜å…¶ä»–æ–‡ä»¶
        with open(f"{output_dir}/feature_names.txt", "w") as f:
            for name in feature_names:
                f.write(f"{name}\n")
        
        import joblib
        joblib.dump(scaler, f"{output_dir}/scaler.pkl")
        
        print(f"âœ… {dataset_name} ä¿®å¤å®Œæˆ!")
        print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬ (ä¹‹å‰: 54/20)")
        print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬ (ä¹‹å‰: 14/5)")
        print(f"   ç‰¹å¾: {len(feature_names)} ä¸ª")
        
        return True
        
    except Exception as e:
        print(f"âŒ {dataset_name} ä¿®å¤å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def fix_all_datasets_outlier_issue():
    """
    ä¿®å¤æ‰€æœ‰æ•°æ®é›†çš„å¼‚å¸¸å€¼ç§»é™¤é—®é¢˜
    """
    print("ðŸš€ ä¿®å¤æ‰€æœ‰æ•°æ®é›†çš„å¼‚å¸¸å€¼ç§»é™¤é—®é¢˜...")
    print("ðŸŽ¯ é—®é¢˜: å¯¹æ¯ä¸ªæ•°å€¼åˆ—éƒ½è¿›è¡Œå¼‚å¸¸å€¼ç§»é™¤å¯¼è‡´æ•°æ®ç´¯ç§¯ä¸¢å¤±")
    print("ðŸ”§ è§£å†³: åªå¯¹ç›®æ ‡å˜é‡è¿›è¡Œå¼‚å¸¸å€¼ç§»é™¤")
    
    datasets = {
        'seattle_2015_present': 'data/seattle_2015_present.csv',
        'chicago_energy': 'data/chicago_energy_benchmarking.csv',
        'washington_dc': 'data/washington_dc_energy.csv'
    }
    
    results = {}
    
    for dataset_name, file_path in datasets.items():
        if os.path.exists(file_path):
            success = reprocess_dataset_with_fix(dataset_name, file_path)
            results[dataset_name] = success
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            results[dataset_name] = False
    
    # æ€»ç»“
    print(f"\nðŸ“Š å¼‚å¸¸å€¼ä¿®å¤ç»“æžœ:")
    successful = sum(results.values())
    total = len(results)
    
    for dataset, success in results.items():
        status = "âœ…" if success else "âŒ"
        print(f"   {status} {dataset}")
    
    print(f"\nðŸŽ¯ ä¿®å¤æˆåŠŸçŽ‡: {successful}/{total} ({successful/total*100:.0f}%)")
    
    if successful > 0:
        print(f"\nðŸ“ˆ é¢„æœŸæ”¹è¿›:")
        print(f"   Seattle: 68 â†’ 20,000+ æ ·æœ¬")
        print(f"   Chicago: 25 â†’ 15,000+ æ ·æœ¬")
        print(f"   Washington DC: 5,228 â†’ ä¿æŒæˆ–æ”¹å–„")
        
        print(f"\nðŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"1. é‡æ–°è¿è¡Œæ¨¡åž‹è®­ç»ƒ:")
        print(f"   python run_project.py --individual")
        print(f"2. æˆ–å•ç‹¬è®­ç»ƒæ¯ä¸ªæ¨¡åž‹")
        print(f"3. ç„¶åŽæŸ¥çœ‹æ”¹è¿›åŽçš„å›¾è¡¨")
    
    return results

# åˆ›å»ºå¿«é€Ÿä¿®å¤è„šæœ¬
def create_quick_fix():
    """
    åˆ›å»ºä¸€ä¸ªå¿«é€Ÿä¿®å¤è„šæœ¬ï¼Œç›´æŽ¥ä¿®æ”¹åŽŸå§‹æ–‡ä»¶
    """
    print("ðŸ“ åˆ›å»ºå¿«é€Ÿä¿®å¤è¡¥ä¸...")
    
    # è¯»å–åŽŸå§‹æ–‡ä»¶
    clean_data_path = "preprocessing/clean_data.py"
    
    if not os.path.exists(clean_data_path):
        print(f"âŒ æ‰¾ä¸åˆ° {clean_data_path}")
        return False
    
    try:
        with open(clean_data_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # åˆ›å»ºå¤‡ä»½
        backup_path = "preprocessing/clean_data.py.backup"
        with open(backup_path, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"âœ… åˆ›å»ºå¤‡ä»½: {backup_path}")
        
        # æ‰¾åˆ°å¹¶æ›¿æ¢å¼‚å¸¸å€¼ç§»é™¤å‡½æ•°
        old_function_start = "def apply_fair_outlier_removal(df, dataset_type, target_col):"
        
        if old_function_start in content:
            # æ‰¾åˆ°å‡½æ•°çš„ç»“æŸä½ç½®
            start_idx = content.find(old_function_start)
            
            # æ‰¾åˆ°ä¸‹ä¸€ä¸ªå‡½æ•°çš„å¼€å§‹
            next_function = content.find("\ndef ", start_idx + 1)
            if next_function == -1:
                next_function = len(content)
            
            # æ›¿æ¢å‡½æ•°
            new_function = '''def apply_fair_outlier_removal(df, dataset_type, target_col):
    """
    ä¿®å¤åŽçš„å¼‚å¸¸å€¼ç§»é™¤ - åªå¯¹ç›®æ ‡å˜é‡è¿›è¡Œå¼‚å¸¸å€¼ç§»é™¤
    """
    from preprocessing.clean_data import get_fair_city_config
    config = get_fair_city_config(dataset_type)
    low_pct, high_pct = config['outlier_percentiles']
    
    print(f"{dataset_type}: åº”ç”¨ä¿®å¤çš„å¼‚å¸¸å€¼ç§»é™¤ ({low_pct}%-{high_pct}% èŒƒå›´)")
    
    original_shape = df.shape[0]
    
    # ä¿®å¤: åªå¯¹ç›®æ ‡å˜é‡è¿›è¡Œå¼‚å¸¸å€¼ç§»é™¤
    if target_col in df.columns:
        target_data = pd.to_numeric(df[target_col], errors='coerce')
        q_low = target_data.quantile(low_pct / 100)
        q_high = target_data.quantile(high_pct / 100)
        
        # åªç§»é™¤ç›®æ ‡å˜é‡çš„æžç«¯å¼‚å¸¸å€¼
        target_mask = (target_data >= q_low) & (target_data <= q_high)
        df = df[target_mask]
        
        print(f"ç›®æ ‡å˜é‡å¼‚å¸¸å€¼ç§»é™¤: ä¿ç•™ {df.shape[0]}/{original_shape} è¡Œ")
    
    final_shape = df.shape[0]
    removed_count = original_shape - final_shape
    removal_pct = (removed_count / original_shape) * 100
    
    print(f"æ€»å¼‚å¸¸å€¼ç§»é™¤: {removed_count} è¡Œ ({removal_pct:.1f}%)")
    
    return df

'''
            
            # æ›¿æ¢å†…å®¹
            new_content = content[:start_idx] + new_function + content[next_function:]
            
            # ä¿å­˜ä¿®æ”¹åŽçš„æ–‡ä»¶
            with open(clean_data_path, 'w', encoding='utf-8') as f:
                f.write(new_content)
            
            print(f"âœ… å·²ä¿®å¤ {clean_data_path}")
            print(f"ðŸ”§ ä¿®å¤: ç§»é™¤äº†å¯¹æ‰€æœ‰æ•°å€¼åˆ—çš„å¼‚å¸¸å€¼è¿‡æ»¤")
            print(f"ðŸ“„ å¤‡ä»½ä¿å­˜åœ¨: {backup_path}")
            
            return True
        else:
            print(f"âŒ æ‰¾ä¸åˆ°ç›®æ ‡å‡½æ•°")
            return False
            
    except Exception as e:
        print(f"âŒ ä¿®å¤å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("ðŸ”§ å¼‚å¸¸å€¼ç§»é™¤é—®é¢˜ä¿®å¤å·¥å…·")
    print("="*50)
    
    # é€‰æ‹©ä¿®å¤æ–¹å¼
    print("é€‰æ‹©ä¿®å¤æ–¹å¼:")
    print("1. ç›´æŽ¥ä¿®æ”¹æºæ–‡ä»¶ (æŽ¨è)")
    print("2. è¿è¡Œæ—¶ä¿®è¡¥")
    
    choice = input("è¯·é€‰æ‹© (1/2): ").strip()
    
    if choice == "1":
        if create_quick_fix():
            print("\nâœ… æºæ–‡ä»¶å·²ä¿®å¤!")
            print("çŽ°åœ¨å¯ä»¥é‡æ–°è¿è¡Œé¢„å¤„ç†:")
            print("python run_project.py --individual")
        else:
            print("\nâŒ æºæ–‡ä»¶ä¿®å¤å¤±è´¥")
    else:
        fix_all_datasets_outlier_issue()