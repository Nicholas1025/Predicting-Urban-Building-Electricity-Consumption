"""
å®Œæ•´çš„æ•°æ®æ£€æŸ¥è„šæœ¬
check_saved_data.py - æ£€æŸ¥é¢„å¤„ç†åä¿å­˜çš„æ•°æ®æ–‡ä»¶
"""

import pandas as pd
import numpy as np
import os
import glob

def check_dataset_files(dataset_name):
    """æ£€æŸ¥å•ä¸ªæ•°æ®é›†çš„æ‰€æœ‰ç›¸å…³æ–‡ä»¶"""
    
    print(f"\n{'='*70}")
    print(f"ğŸ“Š æ£€æŸ¥æ•°æ®é›†: {dataset_name.upper()}")
    print('='*70)
    
    data_dir = f"outputs/{dataset_name}"
    
    if not os.path.exists(data_dir):
        print(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return False
    
    # 1. æ£€æŸ¥åŸºç¡€æ•°æ®æ–‡ä»¶
    print(f"\nğŸ“ åŸºç¡€æ•°æ®æ–‡ä»¶:")
    basic_files = ['X_train.csv', 'X_test.csv', 'y_train.csv', 'y_test.csv']
    
    file_sizes = {}
    for file_name in basic_files:
        file_path = f"{data_dir}/{file_name}"
        if os.path.exists(file_path):
            try:
                df = pd.read_csv(file_path)
                file_sizes[file_name] = len(df)
                features = df.shape[1] if len(df.shape) > 1 else 1
                print(f"   âœ… {file_name}: {len(df)} è¡Œ, {features} åˆ—")
            except Exception as e:
                print(f"   âŒ {file_name}: è¯»å–é”™è¯¯ - {e}")
                file_sizes[file_name] = 0
        else:
            print(f"   âŒ {file_name}: æ–‡ä»¶ä¸å­˜åœ¨")
            file_sizes[file_name] = 0
    
    # 2. æ£€æŸ¥è®­ç»ƒ/æµ‹è¯•æ¯”ä¾‹
    if file_sizes['X_train.csv'] > 0 and file_sizes['X_test.csv'] > 0:
        total_samples = file_sizes['X_train.csv'] + file_sizes['X_test.csv']
        train_ratio = file_sizes['X_train.csv'] / total_samples
        test_ratio = file_sizes['X_test.csv'] / total_samples
        
        print(f"\nğŸ“Š æ•°æ®åˆ†å‰²æ¯”ä¾‹:")
        print(f"   è®­ç»ƒé›†: {file_sizes['X_train.csv']} æ ·æœ¬ ({train_ratio:.1%})")
        print(f"   æµ‹è¯•é›†: {file_sizes['X_test.csv']} æ ·æœ¬ ({test_ratio:.1%})")
        print(f"   æ€»è®¡: {total_samples} æ ·æœ¬")
        
        # æ£€æŸ¥æ¯”ä¾‹æ˜¯å¦åˆç†
        if 0.15 <= test_ratio <= 0.25:
            print(f"   âœ… åˆ†å‰²æ¯”ä¾‹æ­£å¸¸")
        else:
            print(f"   âš ï¸ åˆ†å‰²æ¯”ä¾‹å¼‚å¸¸ (æœŸæœ›æµ‹è¯•é›†å 15-25%)")
    
    # 3. æ£€æŸ¥ç›®æ ‡å˜é‡
    print(f"\nğŸ¯ ç›®æ ‡å˜é‡æ£€æŸ¥:")
    if file_sizes['y_train.csv'] > 0:
        try:
            y_train = pd.read_csv(f"{data_dir}/y_train.csv").iloc[:, 0]
            print(f"   è®­ç»ƒç›®æ ‡: {len(y_train)} å€¼")
            print(f"   èŒƒå›´: [{y_train.min():.2f}, {y_train.max():.2f}]")
            print(f"   å‡å€¼: {y_train.mean():.2f}")
            print(f"   æ ‡å‡†å·®: {y_train.std():.2f}")
        except Exception as e:
            print(f"   âŒ ç›®æ ‡å˜é‡è¯»å–é”™è¯¯: {e}")
    
    if file_sizes['y_test.csv'] > 0:
        try:
            y_test = pd.read_csv(f"{data_dir}/y_test.csv").iloc[:, 0]
            print(f"   æµ‹è¯•ç›®æ ‡: {len(y_test)} å€¼")
            print(f"   èŒƒå›´: [{y_test.min():.2f}, {y_test.max():.2f}]")
        except Exception as e:
            print(f"   âŒ æµ‹è¯•ç›®æ ‡è¯»å–é”™è¯¯: {e}")
    
    # 4. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    print(f"\nğŸ¤– æ¨¡å‹æ–‡ä»¶æ£€æŸ¥:")
    model_files = ['model_xgb.pkl', 'model_rf.pkl', 'model_svr.pkl']
    models_dir = f"{data_dir}/models"
    
    model_count = 0
    if os.path.exists(models_dir):
        for model_file in model_files:
            model_path = f"{models_dir}/{model_file}"
            if os.path.exists(model_path):
                size_mb = os.path.getsize(model_path) / (1024*1024)
                print(f"   âœ… {model_file}: {size_mb:.2f} MB")
                model_count += 1
            else:
                print(f"   âŒ {model_file}: ä¸å­˜åœ¨")
    else:
        print(f"   âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {models_dir}")
    
    # 5. æ£€æŸ¥é¢„æµ‹æ–‡ä»¶
    print(f"\nğŸ“ˆ é¢„æµ‹æ–‡ä»¶æ£€æŸ¥:")
    prediction_files = ['predictions_xgb.csv', 'predictions_rf.csv', 'predictions_svr.csv']
    
    prediction_count = 0
    for pred_file in prediction_files:
        pred_path = f"{data_dir}/{pred_file}"
        if os.path.exists(pred_path):
            try:
                pred_df = pd.read_csv(pred_path)
                prediction_count += 1
                print(f"   âœ… {pred_file}: {len(pred_df)} é¢„æµ‹å€¼")
                
                # æ£€æŸ¥é¢„æµ‹å€¼æ˜¯å¦åˆç†
                predictions = pred_df.iloc[:, 0]
                if predictions.notna().sum() == len(predictions):
                    print(f"       èŒƒå›´: [{predictions.min():.2f}, {predictions.max():.2f}]")
                else:
                    nan_count = predictions.isna().sum()
                    print(f"       âš ï¸ åŒ…å« {nan_count} ä¸ªNaNå€¼")
                    
            except Exception as e:
                print(f"   âŒ {pred_file}: è¯»å–é”™è¯¯ - {e}")
        else:
            print(f"   âŒ {pred_file}: ä¸å­˜åœ¨")
    
    # 6. æ£€æŸ¥å›¾è¡¨æ–‡ä»¶
    print(f"\nğŸ“Š å›¾è¡¨æ–‡ä»¶æ£€æŸ¥:")
    charts_dir = f"{data_dir}/charts"
    chart_count = 0
    
    if os.path.exists(charts_dir):
        chart_files = glob.glob(f"{charts_dir}/*.png")
        chart_count = len(chart_files)
        print(f"   æ‰¾åˆ° {chart_count} ä¸ªå›¾è¡¨æ–‡ä»¶:")
        
        important_charts = [
            'predicted_vs_actual_all_models.png',
            'model_comparison_metrics.png',
            'feature_importance_xgb.png',
            'feature_importance_rf.png'
        ]
        
        for chart_name in important_charts:
            chart_path = f"{charts_dir}/{chart_name}"
            if os.path.exists(chart_path):
                size_kb = os.path.getsize(chart_path) / 1024
                print(f"   âœ… {chart_name}: {size_kb:.1f} KB")
            else:
                print(f"   âŒ {chart_name}: ä¸å­˜åœ¨")
    else:
        print(f"   âŒ å›¾è¡¨ç›®å½•ä¸å­˜åœ¨: {charts_dir}")
    
    # 7. æ£€æŸ¥è¯„ä¼°ç»“æœ
    print(f"\nğŸ“‹ è¯„ä¼°ç»“æœæ£€æŸ¥:")
    results_file = f"{data_dir}/model_evaluation_results.csv"
    
    if os.path.exists(results_file):
        try:
            results_df = pd.read_csv(results_file)
            print(f"   âœ… è¯„ä¼°ç»“æœ: {len(results_df)} ä¸ªæ¨¡å‹")
            
            for _, row in results_df.iterrows():
                model_name = row['Model']
                r2_score = row['R2']
                rmse = row['RMSE']
                print(f"       {model_name}: RÂ² = {r2_score:.4f}, RMSE = {rmse:.2f}")
                
        except Exception as e:
            print(f"   âŒ è¯„ä¼°ç»“æœè¯»å–é”™è¯¯: {e}")
    else:
        print(f"   âŒ è¯„ä¼°ç»“æœæ–‡ä»¶ä¸å­˜åœ¨")
    
    # 8. æ•°æ®é›†å¥åº·çŠ¶å†µæ€»ç»“
    print(f"\nğŸ’Š æ•°æ®é›†å¥åº·çŠ¶å†µ:")
    health_score = 0
    max_score = 7
    
    # åŸºç¡€æ•°æ®å®Œæ•´æ€§
    if all(file_sizes[f] > 0 for f in basic_files):
        health_score += 2
        print(f"   âœ… åŸºç¡€æ•°æ®å®Œæ•´ (+2åˆ†)")
    else:
        missing = [f for f in basic_files if file_sizes[f] == 0]
        print(f"   âŒ åŸºç¡€æ•°æ®ç¼ºå¤±: {missing}")
    
    # æ¨¡å‹è®­ç»ƒå®Œæˆåº¦
    if model_count >= 2:
        health_score += 1
        print(f"   âœ… æ¨¡å‹è®­ç»ƒå……åˆ† (+1åˆ†)")
    else:
        print(f"   âš ï¸ æ¨¡å‹è®­ç»ƒä¸è¶³ ({model_count}/3)")
    
    # é¢„æµ‹ç”Ÿæˆå®Œæˆåº¦
    if prediction_count >= 2:
        health_score += 1
        print(f"   âœ… é¢„æµ‹ç”Ÿæˆå……åˆ† (+1åˆ†)")
    else:
        print(f"   âš ï¸ é¢„æµ‹ç”Ÿæˆä¸è¶³ ({prediction_count}/3)")
    
    # å›¾è¡¨ç”Ÿæˆå®Œæˆåº¦
    if chart_count >= 3:
        health_score += 1
        print(f"   âœ… å›¾è¡¨ç”Ÿæˆå……åˆ† (+1åˆ†)")
    else:
        print(f"   âš ï¸ å›¾è¡¨ç”Ÿæˆä¸è¶³ ({chart_count} ä¸ª)")
    
    # è¯„ä¼°å®Œæˆåº¦
    if os.path.exists(results_file):
        health_score += 1
        print(f"   âœ… è¯„ä¼°å®Œæˆ (+1åˆ†)")
    else:
        print(f"   âŒ è¯„ä¼°æœªå®Œæˆ")
    
    # æ•°æ®é‡åˆç†æ€§
    total_samples = file_sizes.get('X_train.csv', 0) + file_sizes.get('X_test.csv', 0)
    if total_samples > 1000:
        health_score += 1
        print(f"   âœ… æ•°æ®é‡å……è¶³ (+1åˆ†)")
    elif total_samples > 100:
        print(f"   âš ï¸ æ•°æ®é‡ä¸€èˆ¬ ({total_samples} æ ·æœ¬)")
    else:
        print(f"   âŒ æ•°æ®é‡ä¸è¶³ ({total_samples} æ ·æœ¬)")
    
    # å¥åº·çŠ¶å†µè¯„çº§
    health_percentage = (health_score / max_score) * 100
    
    if health_percentage >= 85:
        status = "ğŸŸ¢ ä¼˜ç§€"
    elif health_percentage >= 70:
        status = "ğŸŸ¡ è‰¯å¥½"
    elif health_percentage >= 50:
        status = "ğŸŸ  ä¸€èˆ¬"
    else:
        status = "ğŸ”´ éœ€è¦ä¿®å¤"
    
    print(f"\nğŸ¥ æ€»ä½“å¥åº·åº¦: {health_score}/{max_score} ({health_percentage:.0f}%) - {status}")
    
    return health_score >= max_score * 0.5

def check_all_datasets():
    """æ£€æŸ¥æ‰€æœ‰æ•°æ®é›†"""
    
    print("ğŸ” å¼€å§‹å®Œæ•´æ•°æ®æ£€æŸ¥...")
    print("æ£€æŸ¥é¢„å¤„ç†åä¿å­˜çš„æ‰€æœ‰æ•°æ®æ–‡ä»¶")
    
    datasets = ['seattle_2015_present', 'chicago_energy', 'washington_dc']
    
    # æ£€æŸ¥outputsç›®å½•
    if not os.path.exists('outputs'):
        print("âŒ outputsç›®å½•ä¸å­˜åœ¨!")
        print("è¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†å’Œæ¨¡å‹è®­ç»ƒ")
        return
    
    print(f"âœ… æ‰¾åˆ°outputsç›®å½•")
    
    # æ£€æŸ¥æ¯ä¸ªæ•°æ®é›†
    healthy_datasets = 0
    total_datasets = 0
    
    for dataset in datasets:
        if os.path.exists(f"outputs/{dataset}"):
            is_healthy = check_dataset_files(dataset)
            if is_healthy:
                healthy_datasets += 1
            total_datasets += 1
        else:
            print(f"\nâŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: outputs/{dataset}")
    
    # æ€»ä½“æ€»ç»“
    print(f"\n{'='*70}")
    print(f"ğŸ¯ æ€»ä½“æ£€æŸ¥ç»“æœ")
    print('='*70)
    
    print(f"ğŸ“Š æ•°æ®é›†çŠ¶æ€:")
    print(f"   æ€»æ•°æ®é›†: {len(datasets)}")
    print(f"   å·²å¤„ç†: {total_datasets}")
    print(f"   å¥åº·: {healthy_datasets}")
    print(f"   æˆåŠŸç‡: {healthy_datasets/len(datasets)*100:.1f}%")
    
    if healthy_datasets == len(datasets):
        print(f"ğŸ‰ æ‰€æœ‰æ•°æ®é›†éƒ½å¥åº·!")
        print(f"ğŸ“ˆ å¯ä»¥æ­£å¸¸æŸ¥çœ‹dashboardå’Œç»“æœ")
    elif healthy_datasets > 0:
        print(f"âš ï¸ éƒ¨åˆ†æ•°æ®é›†æœ‰é—®é¢˜")
        print(f"ğŸ”§ å»ºè®®é‡æ–°è¿è¡Œæœ‰é—®é¢˜çš„æ•°æ®é›†")
    else:
        print(f"âŒ æ‰€æœ‰æ•°æ®é›†éƒ½æœ‰é—®é¢˜")
        print(f"ğŸ”§ å»ºè®®é‡æ–°è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹")
    
    # ç»™å‡ºå…·ä½“å»ºè®®
    print(f"\nğŸ’¡ å»ºè®®:")
    print(f"1. å¦‚æœåŸºç¡€æ•°æ®ç¼ºå¤± â†’ é‡æ–°è¿è¡Œé¢„å¤„ç†")
    print(f"2. å¦‚æœæ¨¡å‹ç¼ºå¤± â†’ é‡æ–°è¿è¡Œæ¨¡å‹è®­ç»ƒ")
    print(f"3. å¦‚æœå›¾è¡¨ç¼ºå¤± â†’ é‡æ–°è¿è¡Œè¯„ä¼°è„šæœ¬")
    print(f"4. å¦‚æœæ•°æ®é‡å¤ªå°‘ â†’ æ£€æŸ¥åŸå§‹æ•°æ®æ–‡ä»¶")

def check_specific_issue():
    """ä¸“é—¨æ£€æŸ¥ä½ é‡åˆ°çš„å›¾è¡¨æ•°æ®ç‚¹å°‘çš„é—®é¢˜"""
    
    print(f"\n{'='*70}")
    print(f"ğŸ” ä¸“é¡¹æ£€æŸ¥: å›¾è¡¨æ•°æ®ç‚¹é—®é¢˜")
    print('='*70)
    
    datasets = ['seattle_2015_present', 'chicago_energy', 'washington_dc']
    
    for dataset in datasets:
        print(f"\nğŸ“Š {dataset.upper()} - å›¾è¡¨æ•°æ®æ£€æŸ¥:")
        
        # æ£€æŸ¥æµ‹è¯•é›†å¤§å°
        test_file = f"outputs/{dataset}/y_test.csv"
        if os.path.exists(test_file):
            y_test = pd.read_csv(test_file).iloc[:, 0]
            print(f"   ğŸ¯ æµ‹è¯•é›†å¤§å°: {len(y_test)} ä¸ªæ ·æœ¬")
            
            # æ£€æŸ¥é¢„æµ‹æ–‡ä»¶
            pred_files = ['predictions_xgb.csv', 'predictions_rf.csv', 'predictions_svr.csv']
            
            for pred_file in pred_files:
                pred_path = f"outputs/{dataset}/{pred_file}"
                if os.path.exists(pred_path):
                    try:
                        pred_df = pd.read_csv(pred_path)
                        pred_values = pred_df.iloc[:, 0]
                        
                        print(f"   ğŸ“ˆ {pred_file}: {len(pred_values)} ä¸ªé¢„æµ‹å€¼")
                        
                        # æ£€æŸ¥æ•°æ®åŒ¹é…
                        if len(pred_values) == len(y_test):
                            print(f"       âœ… æ•°æ®é•¿åº¦åŒ¹é…")
                        else:
                            print(f"       âŒ æ•°æ®é•¿åº¦ä¸åŒ¹é…! é¢„æµ‹:{len(pred_values)} vs çœŸå®:{len(y_test)}")
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
                        valid_preds = pred_values.notna().sum()
                        if valid_preds == len(pred_values):
                            print(f"       âœ… æ‰€æœ‰é¢„æµ‹å€¼æœ‰æ•ˆ")
                        else:
                            print(f"       âš ï¸ {len(pred_values) - valid_preds} ä¸ªé¢„æµ‹å€¼æ— æ•ˆ")
                            
                    except Exception as e:
                        print(f"       âŒ è¯»å–é”™è¯¯: {e}")
                else:
                    print(f"   âŒ {pred_file} ä¸å­˜åœ¨")
        else:
            print(f"   âŒ æµ‹è¯•é›†æ–‡ä»¶ä¸å­˜åœ¨")

if __name__ == "__main__":
    check_all_datasets()
    check_specific_issue()
    
    print(f"\nğŸ å®Œæ•´æ£€æŸ¥å®Œæˆ!")
    print(f"ğŸ“‹ å¦‚æœå‘ç°é—®é¢˜ï¼Œè¯·å°†ç»“æœå‘ç»™æˆ‘ï¼Œæˆ‘ä¼šæä¾›å…·ä½“çš„ä¿®å¤æ–¹æ¡ˆ")