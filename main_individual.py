"""
Individual Dataset Analysis for Building Energy Prediction
é’ˆå¯¹æ¯ä¸ªæ•°æ®é›†è¿›è¡Œç‹¬ç«‹åˆ†æï¼Œé¿å…æ•°æ®ä¸å…¼å®¹é—®é¢˜
"""

import os
import sys
import time
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append('.')

def create_individual_directories(dataset_name):
    """ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•"""
    directories = [
        f"outputs/{dataset_name}",
        f"outputs/{dataset_name}/charts",
        f"outputs/{dataset_name}/tables", 
        f"outputs/{dataset_name}/models"
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"âœ“ Created: {directory}")

def process_individual_dataset(dataset_name, file_path):
    """å¤„ç†å•ä¸ªæ•°æ®é›†"""
    print(f"\n{'='*60}")
    print(f"PROCESSING {dataset_name.upper()} DATASET")
    print(f"{'='*60}")
    
    if not os.path.exists(file_path):
        print(f"âŒ File not found: {file_path}")
        return False
    
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        create_individual_directories(dataset_name)
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®©å­æ¨¡å—çŸ¥é“å½“å‰å¤„ç†çš„æ•°æ®é›†
        os.environ['CURRENT_DATASET'] = dataset_name
        os.environ['OUTPUT_PREFIX'] = f"outputs/{dataset_name}"
        
        # Step 1: æ•°æ®é¢„å¤„ç†
        print("ğŸ”„ Step 1: Data Preprocessing...")
        from preprocessing.clean_data import preprocess_data
        
        result = preprocess_data(file_path)
        if result is None:
            print(f"âŒ Preprocessing failed for {dataset_name}")
            return False
            
        X_train, X_test, y_train, y_test, scaler, feature_names = result
        
        # ä¿å­˜é¢„å¤„ç†ç»“æœåˆ°ç‹¬ç«‹æ–‡ä»¶å¤¹
        output_dir = f"outputs/{dataset_name}"
        X_train.to_csv(f"{output_dir}/X_train.csv", index=False)
        X_test.to_csv(f"{output_dir}/X_test.csv", index=False)
        y_train.to_csv(f"{output_dir}/y_train.csv", index=False)
        y_test.to_csv(f"{output_dir}/y_test.csv", index=False)
        
        # ä¿å­˜ç‰¹å¾åå’Œscaler
        with open(f"{output_dir}/feature_names.txt", "w") as f:
            for name in feature_names:
                f.write(f"{name}\n")
        
        import joblib
        joblib.dump(scaler, f"{output_dir}/scaler.pkl")
        
        print(f"âœ… Preprocessing completed for {dataset_name}")
        print(f"   Features: {len(feature_names)}")
        print(f"   Train samples: {len(X_train)}")
        print(f"   Test samples: {len(X_test)}")
        
        # Step 2: æ¨¡å‹è®­ç»ƒ
        print("\nğŸ¤– Step 2: Model Training...")
        train_models_for_dataset(dataset_name)
        
        # Step 3: æ¨¡å‹è¯„ä¼°
        print("\nğŸ“Š Step 3: Model Evaluation...")
        evaluate_models_for_dataset(dataset_name)
        
        # Step 4: åˆ†ç±»æ¨¡å‹ï¼ˆå¦‚æœæ•°æ®è¶³å¤Ÿï¼‰
        if len(X_train) > 500:  # åªæœ‰è¶³å¤Ÿæ•°æ®æ‰åšåˆ†ç±»
            print("\nğŸ¯ Step 4: Classification Models...")
            train_classification_for_dataset(dataset_name, X_train, y_train)
        
        print(f"âœ… {dataset_name} analysis completed successfully!")
        return True
        
    except Exception as e:
        print(f"âŒ Error processing {dataset_name}: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        # æ¸…ç†ç¯å¢ƒå˜é‡
        os.environ.pop('CURRENT_DATASET', None)
        os.environ.pop('OUTPUT_PREFIX', None)

def train_models_for_dataset(dataset_name):
    """ä¸ºå•ä¸ªæ•°æ®é›†è®­ç»ƒæ¨¡å‹"""
    
    # ä¿®æ”¹å·¥ä½œç›®å½•ï¼Œè®©ç°æœ‰çš„è®­ç»ƒè„šæœ¬ä½¿ç”¨æ­£ç¡®çš„è·¯å¾„
    original_dir = os.getcwd()
    output_dir = f"outputs/{dataset_name}"
    
    try:
        # ä¸´æ—¶ä¿®æ”¹è¾“å‡ºè·¯å¾„ç¯å¢ƒå˜é‡
        os.environ['MODEL_OUTPUT_DIR'] = f"{output_dir}/models"
        os.environ['CHART_OUTPUT_DIR'] = f"{output_dir}/charts" 
        os.environ['PRED_OUTPUT_DIR'] = output_dir
        
        # è®­ç»ƒXGBoost
        print("  ğŸš€ Training XGBoost...")
        from models.train_xgboost_individual import main as train_xgb
        train_xgb(dataset_name)
        
        # è®­ç»ƒRandom Forest
        print("  ğŸŒ² Training Random Forest...")
        from models.train_rf_individual import main as train_rf
        train_rf(dataset_name)
        
        # è®­ç»ƒSVR
        print("  âš¡ Training SVR...")
        from models.train_svr_individual import main as train_svr
        train_svr(dataset_name)
        
    except Exception as e:
        print(f"âŒ Model training error: {e}")
    finally:
        # æ¢å¤ç¯å¢ƒå˜é‡
        os.environ.pop('MODEL_OUTPUT_DIR', None)
        os.environ.pop('CHART_OUTPUT_DIR', None) 
        os.environ.pop('PRED_OUTPUT_DIR', None)

def evaluate_models_for_dataset(dataset_name):
    """è¯„ä¼°å•ä¸ªæ•°æ®é›†çš„æ¨¡å‹"""
    try:
        os.environ['EVAL_OUTPUT_DIR'] = f"outputs/{dataset_name}"
        from evaluation.evaluate_models_individual import main as eval_models
        eval_models(dataset_name)
    except Exception as e:
        print(f"âš ï¸  Evaluation error: {e}")
    finally:
        os.environ.pop('EVAL_OUTPUT_DIR', None)

def train_classification_for_dataset(dataset_name, X_train, y_train):
    """ä¸ºå•ä¸ªæ•°æ®é›†è®­ç»ƒåˆ†ç±»æ¨¡å‹"""
    try:
        # åˆ›å»ºåˆ†ç±»æ ‡ç­¾
        from preprocessing.multi_dataset_processor import create_energy_efficiency_labels
        labels = create_energy_efficiency_labels(y_train)
        
        # ä¿å­˜åˆ†ç±»æ•°æ®
        output_dir = f"outputs/{dataset_name}"
        X_train.to_csv(f"{output_dir}/unified_features.csv", index=False)
        labels.to_csv(f"{output_dir}/unified_labels.csv", index=False)
        
        # è®­ç»ƒåˆ†ç±»æ¨¡å‹
        os.environ['CLASS_OUTPUT_DIR'] = output_dir
        from models.train_classification_individual import main as train_class
        train_class(dataset_name)
        
    except Exception as e:
        print(f"âš ï¸  Classification training error: {e}")
    finally:
        os.environ.pop('CLASS_OUTPUT_DIR', None)

def run_individual_analysis():
    """è¿è¡Œç‹¬ç«‹æ•°æ®é›†åˆ†æçš„ä¸»å‡½æ•°"""
    print("="*80)
    print("INDIVIDUAL DATASET ANALYSIS PIPELINE")
    print("="*80)
    print("ğŸ¯ Analyzing each dataset independently to avoid compatibility issues")
    
    # å®šä¹‰æ•°æ®é›†
    datasets = {
        'seattle_2015': 'data/2015-building-energy-benchmarking.csv',
        'seattle_2016': 'data/2016-building-energy-benchmarking.csv',
        'nyc_2021': 'data/energy_disclosure_2021_rows.csv'
    }
    
    results = {}
    total_start_time = time.time()
    
    for dataset_name, file_path in datasets.items():
        start_time = time.time()
        success = process_individual_dataset(dataset_name, file_path)
        end_time = time.time()
        
        results[dataset_name] = {
            'success': success,
            'time': end_time - start_time
        }
        
        print(f"\nğŸ“Š {dataset_name} Summary:")
        print(f"   Status: {'âœ… Success' if success else 'âŒ Failed'}")
        print(f"   Time: {end_time - start_time:.1f} seconds")
    
    total_end_time = time.time()
    total_time = total_end_time - total_start_time
    
    # æœ€ç»ˆæ€»ç»“
    print("\n" + "="*80)
    print("INDIVIDUAL ANALYSIS COMPLETED")
    print("="*80)
    
    successful_datasets = [name for name, result in results.items() if result['success']]
    
    print(f"ğŸ“Š Results Summary:")
    print(f"   Total time: {total_time:.1f} seconds ({total_time/60:.1f} minutes)")
    print(f"   Successful datasets: {len(successful_datasets)}/{len(datasets)}")
    
    for dataset_name, result in results.items():
        status = "âœ…" if result['success'] else "âŒ"
        print(f"   {status} {dataset_name}: {result['time']:.1f}s")
    
    if successful_datasets:
        print(f"\nğŸ‰ Successfully analyzed: {', '.join(successful_datasets)}")
        print(f"ğŸ“ Results saved in: outputs/[dataset_name]/")
        print(f"ğŸŒ Start dashboard to view results!")
    else:
        print(f"\nâš ï¸  No datasets were successfully analyzed")
        print(f"Please check the data files and error messages above")
    
    return results

if __name__ == "__main__":
    run_individual_analysis()